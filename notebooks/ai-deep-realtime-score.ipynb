{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Azure Notebooks](https://notebooks.azure.com/launch.png)](https://notebooks.azure.com/import/gh/microsoft/AI-Utilities)\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/microsoft/AI-Utilities/deep_learning_2?filepath=notebooks%2Fai-deep-realtime-score.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/microsoft/AI-Utilities/blob/deep_learning_2/notebooks/ai-deep-realtime-score.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deploy-Solution\" data-toc-modified-id=\"Deploy-Solution-1\">Deploy Solution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Configuration\" data-toc-modified-id=\"Create-Configuration-1.1\">Create Configuration</a></span></li><li><span><a href=\"#Create-Train.py\" data-toc-modified-id=\"Create-Train.py-1.2\">Create Train.py</a></span></li><li><span><a href=\"#Create-Score.py\" data-toc-modified-id=\"Create-Score.py-1.3\">Create Score.py</a></span></li><li><span><a href=\"#Deploy-to-Azure-Kubernetes-Service-with-Azure-ML\" data-toc-modified-id=\"Deploy-to-Azure-Kubernetes-Service-with-Azure-ML-1.4\">Deploy to Azure Kubernetes Service with Azure ML</a></span></li></ul></li><li><span><a href=\"#Deploy-Services\" data-toc-modified-id=\"Deploy-Services-2\">Deploy Services</a></span><ul class=\"toc-item\"><li><span><a href=\"#Machine-Learning-Studio\" data-toc-modified-id=\"Machine-Learning-Studio-2.1\">Machine Learning Studio</a></span></li><li><span><a href=\"#Kubernetes\" data-toc-modified-id=\"Kubernetes-2.2\">Kubernetes</a></span></li><li><span><a href=\"#Application-Insights\" data-toc-modified-id=\"Application-Insights-2.3\">Application Insights</a></span><ul class=\"toc-item\"><li><span><a href=\"#Main\" data-toc-modified-id=\"Main-2.3.1\">Main</a></span></li><li><span><a href=\"#Availability\" data-toc-modified-id=\"Availability-2.3.2\">Availability</a></span></li><li><span><a href=\"#Performance-Dashboard\" data-toc-modified-id=\"Performance-Dashboard-2.3.3\">Performance Dashboard</a></span></li><li><span><a href=\"#Load-Test\" data-toc-modified-id=\"Load-Test-2.3.4\">Load Test</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "init_cell": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deploy Solution\n",
    "\n",
    "### Sample Configuration Widget\n",
    "![Configuration Widget](https://raw.githubusercontent.com/microsoft/AI-Utilities/master/docs/conda_ui.png)\n",
    "\n",
    "Run the following code to produce the configuration widget. Enter configuration settings, or upload an existing project.yml.\n",
    "\n",
    "### Create Configuration Widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T06:15:04.044101Z",
     "start_time": "2020-03-06T06:14:15.113616Z"
    },
    "init_cell": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from azure_utils.notebook_widgets.notebook_configuration_widget import get_configuration_widget\n",
    "\n",
    "project_configuration = \"notebook_project.yml\"\n",
    "configuration_widget = get_configuration_widget(project_configuration)\n",
    "configuration_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create Train.py\n",
    "\n",
    "The following code trains a model and saves it to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile script/train_dl.py\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import initializers\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.layers import Activation\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import add\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "WEIGHTS_PATH = \"https://github.com/adamcasson/resnet152/releases/download/v0.1/resnet152_weights_tf.h5\"\n",
    "WEIGHTS_PATH_NO_TOP = \"https://github.com/adamcasson/resnet152/releases/download/v0.1/resnet152_weights_tf_notop.h5\"\n",
    "\n",
    "def _obtain_input_shape(input_shape,\n",
    "                        default_size,\n",
    "                        min_size,\n",
    "                        data_format,\n",
    "                        require_flatten,\n",
    "                        weights=None):\n",
    "    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n",
    "        if data_format == 'channels_first':\n",
    "            if input_shape[0] not in {1, 3}:\n",
    "                warnings.warn(\n",
    "                    'This model usually expects 1 or 3 input channels. '\n",
    "                    'However, it was passed an input_shape with ' +\n",
    "                    str(input_shape[0]) + ' input channels.')\n",
    "            default_shape = (input_shape[0], default_size, default_size)\n",
    "        else:\n",
    "            if input_shape[-1] not in {1, 3}:\n",
    "                warnings.warn(\n",
    "                    'This model usually expects 1 or 3 input channels. '\n",
    "                    'However, it was passed an input_shape with ' +\n",
    "                    str(input_shape[-1]) + ' input channels.')\n",
    "            default_shape = (default_size, default_size, input_shape[-1])\n",
    "    else:\n",
    "        if data_format == 'channels_first':\n",
    "            default_shape = (3, default_size, default_size)\n",
    "        else:\n",
    "            default_shape = (default_size, default_size, 3)\n",
    "    if weights == 'imagenet' and require_flatten:\n",
    "        if input_shape is not None:\n",
    "            if input_shape != default_shape:\n",
    "                raise ValueError('When setting`include_top=True` '\n",
    "                                 'and loading `imagenet` weights, '\n",
    "                                 '`input_shape` should be ' +\n",
    "                                 str(default_shape) + '.')\n",
    "        return default_shape\n",
    "    if input_shape:\n",
    "        if data_format == 'channels_first':\n",
    "            if input_shape is not None:\n",
    "                if len(input_shape) != 3:\n",
    "                    raise ValueError(\n",
    "                        '`input_shape` must be a tuple of three integers.')\n",
    "                if input_shape[0] != 3 and weights == 'imagenet':\n",
    "                    raise ValueError('The input must have 3 channels; got '\n",
    "                                     '`input_shape=' + str(input_shape) + '`')\n",
    "                if ((input_shape[1] is not None and input_shape[1] < min_size) or\n",
    "                   (input_shape[2] is not None and input_shape[2] < min_size)):\n",
    "                    raise ValueError('Input size must be at least ' +\n",
    "                                     str(min_size) + 'x' + str(min_size) + '; got '\n",
    "                                     '`input_shape=' + str(input_shape) + '`')\n",
    "        else:\n",
    "            if input_shape is not None:\n",
    "                if len(input_shape) != 3:\n",
    "                    raise ValueError(\n",
    "                        '`input_shape` must be a tuple of three integers.')\n",
    "                if input_shape[-1] != 3 and weights == 'imagenet':\n",
    "                    raise ValueError('The input must have 3 channels; got '\n",
    "                                     '`input_shape=' + str(input_shape) + '`')\n",
    "                if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
    "                   (input_shape[1] is not None and input_shape[1] < min_size)):\n",
    "                    raise ValueError('Input size must be at least ' +\n",
    "                                     str(min_size) + 'x' + str(min_size) + '; got '\n",
    "                                     '`input_shape=' + str(input_shape) + '`')\n",
    "    else:\n",
    "        if require_flatten:\n",
    "            input_shape = default_shape\n",
    "        else:\n",
    "            if data_format == 'channels_first':\n",
    "                input_shape = (3, None, None)\n",
    "            else:\n",
    "                input_shape = (None, None, 3)\n",
    "    if require_flatten:\n",
    "        if None in input_shape:\n",
    "            raise ValueError('If `include_top` is True, '\n",
    "                             'you should specify a static `input_shape`. '\n",
    "                             'Got `input_shape=' + str(input_shape) + '`')\n",
    "    return input_shape\n",
    "\n",
    "\n",
    "class Scale(Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights=None,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        beta_init=\"zero\",\n",
    "        gamma_init=\"one\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "        self.beta_init = initializers.get(beta_init)\n",
    "        self.gamma_init = initializers.get(gamma_init)\n",
    "        self.initial_weights = weights\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (int(input_shape[self.axis]),)\n",
    "\n",
    "        self.gamma = K.variable(self.gamma_init(shape), name=\"%s_gamma\" % self.name)\n",
    "        self.beta = K.variable(self.beta_init(shape), name=\"%s_beta\" % self.name)\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(\n",
    "            self.beta, broadcast_shape\n",
    "        )\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
    "        base_config = super(Scale, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    if K.common.image_dim_ordering() == \"tf\":\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base = \"bn\" + str(stage) + block + \"_branch\"\n",
    "    scale_name_base = \"scale\" + str(stage) + block + \"_branch\"\n",
    "\n",
    "    x = Conv2D(nb_filter1, (1, 1), name=conv_name_base + \"2a\", use_bias=False)(\n",
    "        input_tensor\n",
    "    )\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + \"2a\")(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + \"2a\")(x)\n",
    "    x = Activation(\"relu\", name=conv_name_base + \"2a_relu\")(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + \"2b_zeropadding\")(x)\n",
    "    x = Conv2D(\n",
    "        nb_filter2,\n",
    "        (kernel_size, kernel_size),\n",
    "        name=conv_name_base + \"2b\",\n",
    "        use_bias=False,\n",
    "    )(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + \"2b\")(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + \"2b\")(x)\n",
    "    x = Activation(\"relu\", name=conv_name_base + \"2b_relu\")(x)\n",
    "\n",
    "    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + \"2c\", use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + \"2c\")(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + \"2c\")(x)\n",
    "\n",
    "    x = add([x, input_tensor], name=\"res\" + str(stage) + block)\n",
    "    x = Activation(\"relu\", name=\"res\" + str(stage) + block + \"_relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    if K.common.image_dim_ordering() == \"tf\":\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base = \"bn\" + str(stage) + block + \"_branch\"\n",
    "    scale_name_base = \"scale\" + str(stage) + block + \"_branch\"\n",
    "\n",
    "    x = Conv2D(\n",
    "        nb_filter1, (1, 1), strides=strides, name=conv_name_base + \"2a\", use_bias=False\n",
    "    )(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + \"2a\")(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + \"2a\")(x)\n",
    "    x = Activation(\"relu\", name=conv_name_base + \"2a_relu\")(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + \"2b_zeropadding\")(x)\n",
    "    x = Conv2D(\n",
    "        nb_filter2,\n",
    "        (kernel_size, kernel_size),\n",
    "        name=conv_name_base + \"2b\",\n",
    "        use_bias=False,\n",
    "    )(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + \"2b\")(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + \"2b\")(x)\n",
    "    x = Activation(\"relu\", name=conv_name_base + \"2b_relu\")(x)\n",
    "\n",
    "    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + \"2c\", use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + \"2c\")(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + \"2c\")(x)\n",
    "\n",
    "    shortcut = Conv2D(\n",
    "        nb_filter3, (1, 1), strides=strides, name=conv_name_base + \"1\", use_bias=False\n",
    "    )(input_tensor)\n",
    "    shortcut = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + \"1\")(\n",
    "        shortcut\n",
    "    )\n",
    "    shortcut = Scale(axis=bn_axis, name=scale_name_base + \"1\")(shortcut)\n",
    "\n",
    "    x = add([x, shortcut], name=\"res\" + str(stage) + block)\n",
    "    x = Activation(\"relu\", name=\"res\" + str(stage) + block + \"_relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def ResNet152(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    large_input=False,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "):\n",
    "    if weights not in {\"imagenet\", None}:\n",
    "        raise ValueError(\n",
    "            \"The `weights` argument should be either \"\n",
    "            \"`None` (random initialization) or `imagenet` \"\n",
    "            \"(pre-training on ImageNet).\"\n",
    "        )\n",
    "\n",
    "    if weights == \"imagenet\" and include_top and classes != 1000:\n",
    "        raise ValueError(\n",
    "            \"If using `weights` as imagenet with `include_top`\"\n",
    "            \" as true, `classes` should be 1000\"\n",
    "        )\n",
    "\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    if large_input:\n",
    "        img_size = 448\n",
    "    else:\n",
    "        img_size = 224\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(\n",
    "        input_shape,\n",
    "        default_size=img_size,\n",
    "        min_size=197,\n",
    "        data_format=K.image_data_format(),\n",
    "        require_flatten=include_top,\n",
    "    )\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # handle dimension ordering for different backends\n",
    "    if K.common.image_dim_ordering() == \"tf\":\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = ZeroPadding2D((3, 3), name=\"conv1_zeropadding\")(img_input)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name=\"conv1\", use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=\"bn_conv1\")(x)\n",
    "    x = Scale(axis=bn_axis, name=\"scale_conv1\")(x)\n",
    "    x = Activation(\"relu\", name=\"conv1_relu\")(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block=\"a\", strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\"b\")\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\"c\")\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block=\"a\")\n",
    "    for i in range(1, 8):\n",
    "        x = identity_block(x, 3, [128, 128, 512], stage=3, block=\"b\" + str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block=\"a\")\n",
    "    for i in range(1, 36):\n",
    "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\"b\" + str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block=\"a\")\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\"b\")\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\"c\")\n",
    "\n",
    "    if large_input:\n",
    "        x = AveragePooling2D((14, 14), name=\"avg_pool\")(x)\n",
    "    else:\n",
    "        x = AveragePooling2D((7, 7), name=\"avg_pool\")(x)\n",
    "\n",
    "    # include classification layer by default, not included for feature extraction\n",
    "    if include_top:\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, activation=\"softmax\", name=\"fc1000\")(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == \"max\":\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name=\"resnet152\")\n",
    "\n",
    "    # load weights\n",
    "    if weights == \"imagenet\":\n",
    "        if include_top:\n",
    "            weights_path = get_file(\n",
    "                \"resnet152_weights_tf.h5\",\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir=\"models\",\n",
    "                md5_hash=\"cdb18a2158b88e392c0905d47dcef965\",\n",
    "            )\n",
    "        else:\n",
    "            weights_path = get_file(\n",
    "                \"resnet152_weights_tf_notop.h5\",\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir=\"models\",\n",
    "                md5_hash=\"4a90dcdafacbd17d772af1fb44fc2660\",\n",
    "            )\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "        if K.backend() == \"theano\":\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "            if include_top:\n",
    "                maxpool = model.get_layer(name=\"avg_pool\")\n",
    "                shape = maxpool.output_shape[1:]\n",
    "                dense = model.get_layer(name=\"fc1000\")\n",
    "                layer_utils.convert_dense_weights_data_format(\n",
    "                    dense, shape, \"channels_first\"\n",
    "                )\n",
    "\n",
    "        if K.image_data_format() == \"channels_first\" and K.backend() == \"tensorflow\":\n",
    "            warnings.warn(\n",
    "                \"You are using the TensorFlow backend, yet you \"\n",
    "                \"are using the Theano \"\n",
    "                \"image data format convention \"\n",
    "                '(`image_data_format=\"channels_first\"`). '\n",
    "                \"For best performance, set \"\n",
    "                '`image_data_format=\"channels_last\"` in '\n",
    "                \"your Keras config \"\n",
    "                \"at ~/.keras/keras.json.\"\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import warnings\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "        import os\n",
    "        os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "        model = ResNet152(include_top=False, input_shape=(200, 200, 3), pooling=\"avg\", weights=\"imagenet\")\n",
    "        model.save_weights(\"outputs/model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create Score.py\n",
    "The scoring script is used to create a rest service. The model is loaded, and is used to make predictions on incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile source/score.py\n",
    "\n",
    "from azure_utils.machine_learning.training_arg_parsers import default_response\n",
    "from azureml.contrib.services.aml_request import rawhttp\n",
    "\n",
    "def init():\n",
    "    pass\n",
    "\n",
    "@rawhttp\n",
    "def run(request):\n",
    "    if request.method == 'POST':\n",
    "        return default_response(request)\n",
    "    return default_response(request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_py = \"source/score.py\"\n",
    "\n",
    "\n",
    "class MockRequest:\n",
    "    \"\"\"Mock Request Class to create calls to test web service code\"\"\"\n",
    "    method = \"GET\"\n",
    "\n",
    "def test_score_file(score_py):\n",
    "    exec(open(score_py).read())\n",
    "    exec(\"init()\")\n",
    "    exec(\"response = run(MockRequest())\")\n",
    "    exec(\"assert response\")\n",
    "    exec(\"print(str(response.response))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Deploy to Azure Kubernetes Service with Azure ML\n",
    "\n",
    "Train the model locally, and then deploy the web service to an Azure Kubernetes Cluster managed by an Azure Machine Learning Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azure_utils.machine_learning.contexts.realtime_score_context import  DeepRealtimeScore\n",
    "\n",
    "deep_ws, aks_service = DeepRealtimeScore.get_or_or_create(configuration_file=project_configuration,\n",
    "                                                          train_py=\"train_dl.py\", score_py=\"score.py\")\n",
    "deep_ws.workspace_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai-utilities"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "237px",
    "left": "488px",
    "top": "188px",
    "width": "561px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
