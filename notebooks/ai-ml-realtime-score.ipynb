{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% %writefile train.py\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "import joblib\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "\n",
    "from azure_utils.machine_learning.item_selector import ItemSelector\n",
    "\n",
    "if __name__ == '__main__':\n",
    "# \"\"\" Main Method to use with AzureML\"\"\"\n",
    "    # Define the arguments.\n",
    "    parser = argparse.ArgumentParser(description='Fit and evaluate a model based on train-test datasets.')\n",
    "    parser.add_argument('-d', '--train_data', help='the training dataset name', default='balanced_pairs_train.tsv')\n",
    "    parser.add_argument('-t', '--test_data', help='the test dataset name', default='balanced_pairs_test.tsv')\n",
    "    parser.add_argument('-i', '--estimators', help='the number of learner estimators', type=int, default=8000)\n",
    "    parser.add_argument('--min_child_samples', help='the minimum number of samples in a child(leaf)', type=int,\n",
    "                        default=20)\n",
    "    parser.add_argument('-v', '--verbose', help='the verbosity of the estimator', type=int, default=-1)\n",
    "    parser.add_argument('-n', '--ngrams', help='the maximum size of word ngrams', type=int, default=1)\n",
    "    parser.add_argument('-u', '--unweighted', help='do not use instance weights', action='store_true', default=False)\n",
    "    parser.add_argument('-m', '--match', help='the maximum number of duplicate matches', type=int, default=20)\n",
    "    parser.add_argument('--outputs', help='the outputs directory', default='.')\n",
    "    parser.add_argument('--inputs', help='the inputs directory', default='.')\n",
    "    parser.add_argument('-s', '--save', help='save the model', action='store_true', default=True)\n",
    "    parser.add_argument('--model', help='the model file', default='model.pkl')\n",
    "    parser.add_argument('--instances', help='the instances file', default='inst.txt')\n",
    "    parser.add_argument('--labels', help='the labels file', default='labels.txt')\n",
    "    parser.add_argument('-r', '--rank', help='the maximum rank of correct answers', type=int, default=3)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    run = Run.get_context()\n",
    "\n",
    "    # The training and testing datasets.\n",
    "    inputs_path = args.inputs\n",
    "    data_path = os.path.join(inputs_path, args.train_data)\n",
    "    test_path = os.path.join(inputs_path, args.test_data)\n",
    "\n",
    "    # Create the outputs folder.\n",
    "    outputs_path = args.outputs\n",
    "    os.makedirs(outputs_path, exist_ok=True)\n",
    "    model_path = os.path.join(outputs_path, args.model)\n",
    "    instances_path = os.path.join(outputs_path, args.instances)\n",
    "    labels_path = os.path.join(outputs_path, args.labels)\n",
    "\n",
    "    # Load the training data\n",
    "    print('Reading {}'.format(data_path))\n",
    "    train = pd.read_csv(data_path, sep='\\t', encoding='latin1')\n",
    "\n",
    "    # Limit the number of duplicate-original question matches.\n",
    "    train = train[train.n < args.match]\n",
    "\n",
    "    # Define the roles of the columns in the training data.\n",
    "    feature_columns = ['Text_x', 'Text_y']\n",
    "    label_column = 'Label'\n",
    "    duplicates_id_column = 'Id_x'\n",
    "    answer_id_column = 'AnswerId_y'\n",
    "\n",
    "    # Report on the training dataset: the number of rows and the proportion of true matches.\n",
    "    print('train: {:,} rows with {:.2%} matches'.format(\n",
    "        train.shape[0], train[label_column].mean()))\n",
    "\n",
    "    # Compute the instance weights used to correct for class imbalance in training.\n",
    "    weight_column = 'Weight'\n",
    "    if args.unweighted:\n",
    "        weight = pd.Series([1.0], train[label_column].unique())\n",
    "    else:\n",
    "        label_counts = train[label_column].value_counts()\n",
    "        weight = train.shape[0] / (label_counts.shape[0] * label_counts)\n",
    "    train[weight_column] = train[label_column].apply(lambda x: weight[x])\n",
    "\n",
    "    # Collect the unique ids that identify each original question's answer.\n",
    "    labels = sorted(train[answer_id_column].unique())\n",
    "    label_order = pd.DataFrame({'label': labels})\n",
    "\n",
    "    # Collect the parts of the training data by role.\n",
    "    train_x = train[feature_columns]\n",
    "    train_y = train[label_column]\n",
    "    sample_weight = train[weight_column]\n",
    "\n",
    "    # Use the inputs to define the hyperparameters used in training.\n",
    "    n_estimators = args.estimators\n",
    "    min_child_samples = args.min_child_samples\n",
    "    if args.ngrams > 0:\n",
    "        ngram_range = (1, args.ngrams)\n",
    "    else:\n",
    "        ngram_range = None\n",
    "\n",
    "    # Verify that the hyperparameter values are valid.\n",
    "    assert n_estimators > 0\n",
    "    assert min_child_samples > 1\n",
    "    assert isinstance(ngram_range, tuple) and len(ngram_range) == 2\n",
    "    assert 0 < ngram_range[0] <= ngram_range[1]\n",
    "\n",
    "    # Define the pipeline that featurizes the text columns.\n",
    "    featurization = [\n",
    "        (column,\n",
    "         make_pipeline(ItemSelector(column),\n",
    "                       text.TfidfVectorizer(ngram_range=ngram_range)))\n",
    "        for column in feature_columns]\n",
    "    features = FeatureUnion(featurization)\n",
    "\n",
    "    # Define the estimator that learns how to classify duplicate-original question pairs.\n",
    "    estimator = lgb.LGBMClassifier(n_estimators=n_estimators,\n",
    "                                   min_child_samples=min_child_samples,\n",
    "                                   verbose=args.verbose)\n",
    "\n",
    "    # Define the model pipeline as feeding the features into the estimator.\n",
    "    model = Pipeline([\n",
    "        ('features', features),\n",
    "        ('model', estimator)\n",
    "    ])\n",
    "\n",
    "    # Fit the model.\n",
    "    print('Training...')\n",
    "    model.fit(train_x, train_y, model__sample_weight=sample_weight)\n",
    "\n",
    "    # Save the model to a file, and report on its size.\n",
    "    if args.save:\n",
    "        joblib.dump(model, model_path)\n",
    "        print('{} size: {:.2f} MB'.format(model_path, os.path.getsize(model_path) / (2 ** 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import logging\n",
    "\n",
    "def init():\n",
    "    logger = logging.getLogger(\"scoring_script\")\n",
    "    logger.info(\"init\")\n",
    "\n",
    "\n",
    "def run():\n",
    "    logger = logging.getLogger(\"scoring_script\")\n",
    "    logger.info(\"run\")\n",
    "    return json.dumps({'call': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azure_utils.machine_learning.contexts.realtime_score_context import  MLRealtimeScore\n",
    "\n",
    "MLRealtimeScore.get_or_or_create(configuration_file=\"ml-rts.project.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-utilities",
   "language": "python",
   "name": "ai-utilities"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}